{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccfbdf3",
   "metadata": {},
   "source": [
    "### Creating tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77686917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class Vocabulary:\n",
    "    SPLIT_PATTERN = re.compile(r'([,.:;?!_\"()\\']|--|\\s)')\n",
    "    END_OF_TEXT = '<|endoftext|>'\n",
    "    UNKNOWN = '<|unk|>'\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        \n",
    "    def __read_content__(self):\n",
    "        with open(self.filename, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        return content\n",
    "    \n",
    "    def create_vocabulary(self):\n",
    "        content = self.__read_content__()\n",
    "        tokens = self.SPLIT_PATTERN.split(content)\n",
    "        tokens = [token.strip() for token in tokens if token.strip()]\n",
    "        \n",
    "        unique_tokens = sorted(set(tokens))\n",
    "        unique_tokens.extend((self.UNKNOWN, self.END_OF_TEXT))  # Add padding and unknown tokens\n",
    "        return {token: idx for idx, token in enumerate(unique_tokens)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ffc584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV1:\n",
    "    ENCODE_SPLIT_PATTERN = re.compile(r'([,.:;?!_\"()\\']|--|\\s)')\n",
    "    DECODE_SUB_PATTERN = re.compile(r'\\s+([,.?!\"()\\'])')\n",
    "    \n",
    "    def __init__(self, vocabulary_or_file_name):\n",
    "        if isinstance(vocabulary_or_file_name, str):\n",
    "            creator = Vocabulary(vocabulary_or_file_name)\n",
    "            self.vocabulary = creator.create_vocabulary()\n",
    "        elif isinstance(vocabulary_or_file_name, dict):\n",
    "            self.vocabulary = vocabulary_or_file_name\n",
    "        else:\n",
    "            raise ValueError(\"Vocabulary must be a filename or a dictionary.\")\n",
    "        self.reverse_vocabulary = {idx: token for token, idx in self.vocabulary.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = self.ENCODE_SPLIT_PATTERN.split(text)\n",
    "        preprocessed = [word.strip() for word in preprocessed if word.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.vocabulary else Vocabulary.UNKNOWN \n",
    "            for item in preprocessed\n",
    "        ]\n",
    "        return [self.vocabulary.get(word, -1) for word in preprocessed]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = ' '.join(self.reverse_vocabulary.get(idx, '') for idx in ids)\n",
    "        return self.DECODE_SUB_PATTERN.sub(r'\\1', text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e4f0aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: It's the last he painted, you know,\"  Mrs. Gisburn said with pardonable pride.\n",
      "Encoded tokens: [56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "Decoded text: It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n",
      "\n",
      "\n",
      "Original text: It's the last he painted, you know, <|endoftext|> Mrs. Gisburn said with pardonable pride.\n",
      "Encoded tokens: [56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1131, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "Decoded text: It' s the last he painted, you know, <|endoftext|> Mrs. Gisburn said with pardonable pride.\n",
      "\n",
      "\n",
      "Original text: Dawud says Hello <|endoftext|> In the sunlit terraces of the palace.\n",
      "Encoded tokens: [1130, 858, 1130, 1131, 55, 988, 956, 984, 722, 988, 1130, 7]\n",
      "Decoded text: <|unk|> says <|unk|> <|endoftext|> In the sunlit terraces of the <|unk|>.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TokenizerTeset:\n",
    "    def __init__(self, filename):\n",
    "        self.tokenizer = TokenizerV1(filename)\n",
    "    \n",
    "    def test_tokenizer(self, test_texts):\n",
    "        test_text = test_texts if isinstance(test_texts, str) else f' {Vocabulary.END_OF_TEXT} '.join(test_texts)\n",
    "        encoded = self.tokenizer.encode(test_text)\n",
    "        decoded = self.tokenizer.decode(encoded)\n",
    "        \n",
    "        print(f\"Original text: {test_text}\")\n",
    "        print(f\"Encoded tokens: {encoded}\")\n",
    "        print(f\"Decoded text: {decoded}\\n\\n\")\n",
    "\n",
    "tester = TokenizerTeset('verdict.txt')\n",
    "tester.test_tokenizer( \"\"\"It's the last he painted, you know,\"  Mrs. Gisburn said with pardonable pride.\"\"\")\n",
    "tester.test_tokenizer([  \"It's the last he painted, you know,\",\"Mrs. Gisburn said with pardonable pride.\"])\n",
    "tester.test_tokenizer([ 'Dawud says Hello',  'In the sunlit terraces of the palace.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9212fcaf",
   "metadata": {},
   "source": [
    "## BYTE PAIR ENCODING (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6084edc2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests>=2.26.0 (from tiktoken)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Installing collected packages: urllib3, regex, idna, charset-normalizer, certifi, requests, tiktoken\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [tiktoken]6/7\u001b[0m [tiktoken]\n",
      "\u001b[1A\u001b[2KSuccessfully installed certifi-2025.4.26 charset-normalizer-3.4.2 idna-3.10 regex-2024.11.6 requests-2.32.3 tiktoken-0.9.0 urllib3-2.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10b43f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n",
      "\n",
      "Encoding:\n",
      "Number of tokens: 88\n",
      "Tokens: [15496, 11, 995, 0, 770, 318, 257, 1332, 286, 262, 256, 1134, 30001, 5888, 13, 220, 50256, 1026, 318, 3562, 284, 11241, 1096, 2420, 18306, 329, 779, 351, 4946, 20185, 338, 402, 11571, 4981, 13, 30642, 1634, 318, 262, 1429, 286, 23202, 2420, 656, 16326, 11, 543, 389, 262, 4096, 4991, 286, 3616, 13, 220, 50256, 1212, 5888, 6971, 2972, 2207, 375, 654, 11, 1390, 402, 11571, 12, 17, 290, 402, 11571, 12, 18, 13, 40, 1101, 422, 2773, 2954, 593, 27271, 11, 290, 314, 1842, 19617, 0]\n",
      "\n",
      "Decoding:\n",
      "Decoded text: Hello, world! This is a test of the tiktoken library. <|endoftext|>It is designed to tokenize text efficiently for use with OpenAI's GPT models.Tokenization is the process of converting text into tokens, which are the basic units of meaning. <|endoftext|>This library supports various encodings, including GPT-2 and GPT-3.I'm from SomeunkownPlace, and I love coding!\n",
      "Decoded text matches original: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "print(f\"tiktoken version: {tiktoken.__version__}\\n\")\n",
    "\n",
    "# Suppoorted encoding  -  https://github.com/openai/tiktoken/blob/main/tiktoken/model.py\n",
    "# example: gpt2, o200k_base, cl100k_base, r50k_base, p50k_base, p50k_edit, r50k_edit\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text =  ( \"Hello, world! This is a test of the tiktoken library. <|endoftext|>\"\n",
    "            \"It is designed to tokenize text efficiently for use with OpenAI's GPT models.\"\n",
    "            \"Tokenization is the process of converting text into tokens, which are the basic units of meaning. <|endoftext|>\"\n",
    "            \"This library supports various encodings, including GPT-2 and GPT-3.\"\n",
    "            \"I'm from SomeunkownPlace, and I love coding!\"\n",
    ")\n",
    "\n",
    "# encoding text\n",
    "tokens = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "print(f'Encoding:\\nNumber of tokens: {len(tokens)}\\nTokens: {tokens}\\n')\n",
    "\n",
    "# decoding tokens back to text\n",
    "decoded_t =  tokenizer.decode(tokens)\n",
    "print(f'Decoding:\\nDecoded text: {decoded_t}\\nDecoded text matches original: {decoded_t == text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3678b91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
